{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a096dc-e082-4f2c-aa9e-488aeeefc0d3",
   "metadata": {},
   "source": [
    "# Advanced RAG Pipeline with CrewAI: Analyzing Research Papers\n",
    " \n",
    "This notebook demonstrates an advanced Retrieval-Augmented Generation (RAG) pipeline using CrewAI to analyze research papers because although I am not a scientist, I love science and wish to understand impressive new published papers. We will, rightfully, use the industry shaking \"Attention Is All You Need\" paper as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb4e27b-1049-4f4e-98b9-a71aff4bd308",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c44c4dab-e4ad-48a3-8a6c-41cc57f67fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from uuid import uuid4\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain import hub\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "\n",
    "from crewai import Agent, Task, Crew\n",
    "from crewai_tools import BaseTool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a52af4c-6e51-4a0b-9637-b134dc3d9cd1",
   "metadata": {},
   "source": [
    "## Load Environment Variables and Set Up Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c0c5c0-c9a1-4f9f-a78f-872a698fbe77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43a0433e-d72a-495e-81ac-b6c9b68bec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Set up API keys and configurations\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "QDRANT_API_KEY = os.getenv('QDRANT_API_KEY')\n",
    "QDRANT_URL = os.getenv('QDRANT_URL')\n",
    "COLLECTION_NAME = \"attention_pdf_paper_1\"\n",
    "\n",
    "# Ensure all necessary environment variables are set\n",
    "assert all([OPENAI_API_KEY, QDRANT_API_KEY, QDRANT_URL]), \"Please set all required environment variables.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e222ac-ea14-4ae9-83bf-b88495f98686",
   "metadata": {},
   "source": [
    "## Document Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7be61a6-1a91-4f47-9eda-de0fd074a92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded and split into 92 chunks.\n"
     ]
    }
   ],
   "source": [
    "def load_and_split_document(file_path: str, chunk_size: int = 512, chunk_overlap: int = 50) -> list:\n",
    "    \"\"\"\n",
    "    Load a PDF document and split it into chunks for processing.\n",
    "    \n",
    "    Args:\n",
    "    file_path (str): Path to the PDF file.\n",
    "    chunk_size (int): Size of each text chunk.\n",
    "    chunk_overlap (int): Overlap between chunks.\n",
    "    \n",
    "    Returns:\n",
    "    list: List of document chunks.\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return loader.load_and_split(text_splitter)\n",
    "\n",
    "# Load and split the document\n",
    "document = load_and_split_document(\"attention.pdf\")\n",
    "print(f\"Document loaded and split into {len(document)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b376fafc-adf4-44b4-ab56-77754c3f9821",
   "metadata": {},
   "source": [
    "## Set Up Qdrant Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cae2158-c35a-4476-9150-ff58a4d5cf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'attention_pdf_paper_1' created successfully.\n"
     ]
    }
   ],
   "source": [
    "def setup_qdrant_client():\n",
    "    \"\"\"\n",
    "    Set up and return a Qdrant client for vector storage.\n",
    "    \"\"\"\n",
    "    return QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "\n",
    "def create_qdrant_collection(client: QdrantClient, collection_name: str, vector_size: int = 1536):\n",
    "    \"\"\"\n",
    "    Create a new collection in Qdrant for storing document vectors.\n",
    "    \n",
    "    Args:\n",
    "    client (QdrantClient): Initialized Qdrant client.\n",
    "    collection_name (str): Name of the collection to create.\n",
    "    vector_size (int): Size of the vector embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use cosine similarity to focus on the direction of vectors rather than their magnitude.\n",
    "    # crucial when comparing features like text embeddings where the magnitude is less important than the angle between vectors\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config={\n",
    "            \"content\": VectorParams(size=vector_size, distance=Distance.COSINE)\n",
    "        }\n",
    "    )\n",
    "    print(f\"Collection '{collection_name}' created successfully.\")\n",
    "\n",
    "# Set up Qdrant client and create collection\n",
    "qdrant_client = setup_qdrant_client()\n",
    "create_qdrant_collection(qdrant_client, COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "215c8fd4-59f4-413a-a69c-95f64ae55eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 document chunks embedded and stored in Qdrant.\n"
     ]
    }
   ],
   "source": [
    "# ## Document Embedding and Storage\n",
    "def embed_and_store_documents(documents: list, client: QdrantClient, collection_name: str):\n",
    "    \"\"\"\n",
    "    Embed documents and store them in the Qdrant collection.\n",
    "    \n",
    "    Args:\n",
    "    documents (list): List of document chunks to embed and store.\n",
    "    client (QdrantClient): Initialized Qdrant client.\n",
    "    collection_name (str): Name of the collection to store embeddings.\n",
    "    \"\"\"\n",
    "    embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "    \n",
    "    chunked_metadata = []\n",
    "    for doc in documents:\n",
    "        id = str(uuid4())\n",
    "        content_vector = embedding.embed_documents([doc.page_content])[0]\n",
    "        \n",
    "        metadata = PointStruct(\n",
    "            id=id,\n",
    "            vector={\"content\": content_vector},\n",
    "            payload={\n",
    "                \"page_content\": doc.page_content,\n",
    "                \"metadata\": {\n",
    "                    \"id\": id,\n",
    "                    \"source\": doc.metadata[\"source\"],\n",
    "                    \"page\": doc.metadata[\"page\"],\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        chunked_metadata.append(metadata)\n",
    "    \n",
    "    client.upsert(collection_name=collection_name, wait=True, points=chunked_metadata)\n",
    "    print(f\"{len(chunked_metadata)} document chunks embedded and stored in Qdrant.\")\n",
    "\n",
    "# Embed and store documents\n",
    "embed_and_store_documents(document, qdrant_client, COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f0de18-7c83-4021-a01b-b688bf96de1f",
   "metadata": {},
   "source": [
    "## Set Up RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe1f575a-206a-43cc-aff5-41f898bb7a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 0.3.0. An updated version of the class exists in the langchain-qdrant package and should be used instead. To use it run `pip install -U langchain-qdrant` and import as `from langchain_qdrant import Qdrant`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "def setup_rag_pipeline():\n",
    "    \"\"\"\n",
    "    Set up the Retrieval-Augmented Generation (RAG) pipeline.\n",
    "    \n",
    "    Returns:\n",
    "    RetrievalQA: Initialized RetrievalQA object for question answering.\n",
    "    \"\"\"\n",
    "    embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "    vectorstore = Qdrant(client=qdrant_client,\n",
    "                         collection_name=COLLECTION_NAME,\n",
    "                         embeddings=embedding,\n",
    "                         vector_name=\"content\")\n",
    "    \n",
    "    retriever = vectorstore.as_retriever()\n",
    "    \n",
    "    template = \"\"\"\n",
    "    You are an AI assistant specializing in analyzing research papers.\n",
    "    Use the following retrieved context to answer the question.\n",
    "    If you can't answer the question based on the context, say you don't know.\n",
    "    \n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    llm4o = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n",
    "                     temperature=0.0,\n",
    "                     model=\"gpt-4o\",\n",
    "                     max_tokens=512)\n",
    "    \n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm4o,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "\n",
    "# Set up RAG pipeline\n",
    "qa_chain = setup_rag_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5467838b-738a-43b1-b931-cfd3fe8eacd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-attention, also known as intra-attention, is a mechanism that relates different positions within a single sequence to compute a representation of that sequence. It has been effectively used in tasks like reading comprehension, summarization, and sentence representation.\n"
     ]
    }
   ],
   "source": [
    "print(qa_chain.invoke({\"query\": \"In a short, direct, jargonless, and structured manner, explain what is the self attention mechanism?\"})[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf5cdda-3120-40e2-a5ca-11c036bce796",
   "metadata": {},
   "source": [
    "### This approach can be adapted to analyze various types of research papers or documents, providing both detailed analysis and accessible summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55913728-69b8-4c74-93ae-378128e368fa",
   "metadata": {},
   "source": [
    "## CrewAI Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63da8f44-ba37-4328-b0e1-c95215780231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchPaperAnalysisTool(BaseTool):\n",
    "    \"\"\"Custom tool for analyzing research papers using the RAG pipeline.\"\"\"\n",
    "    \n",
    "    name: str = \"Research Paper Analysis\"\n",
    "    description: str = \"Analyzes the content of a research paper using a RAG pipeline.\"\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        \"\"\"Run the research paper analysis tool.\"\"\"\n",
    "        result = qa_chain({\"query\": query})\n",
    "        return result[\"result\"]\n",
    "\n",
    "# Create an instance of the custom tool\n",
    "research_tool = ResearchPaperAnalysisTool()\n",
    "\n",
    "# Define CrewAI agents\n",
    "researcher = Agent(\n",
    "    role=\"Senior Computer Science Researcher\",\n",
    "    goal=\"Conduct a comprehensive analysis of the provided research paper\",\n",
    "    backstory=\"\"\"You are a renowned scientist specializing in Machine Learning and Artificial Intelligence.\n",
    "    Your expertise allows you to dissect complex research papers and extract key insights.\"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    tools=[research_tool]\n",
    ")\n",
    "\n",
    "writer = Agent(\n",
    "    role=\"Technical Writer and Educator\",\n",
    "    goal=\"Create an accessible summary of the research paper for graduate students\",\n",
    "    backstory=\"\"\"You are an experienced technical writer with a knack for explaining complex concepts in simple engaging terms.\n",
    "    Your goal is to make cutting-edge research accessible to a wider audience.\"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False\n",
    ")\n",
    "\n",
    "# Define CrewAI tasks\n",
    "analysis_task = Task(\n",
    "    description=\"\"\"Conduct an in-depth analysis of the 'Attention Is All You Need' paper.\n",
    "    Focus on the key contributions, architectural details, and potential impacts on the field of NLP.\"\"\",\n",
    "    expected_output=\"\"\"A comprehensive analysis report covering:\n",
    "    1. Key innovations of the Transformer architecture\n",
    "    2. Detailed explanation of the self-attention mechanism\n",
    "    3. Comparison with previous architectures (e.g., RNNs, LSTMs)\n",
    "    4. Potential implications for future NLP research and applications\"\"\",\n",
    "    agent=researcher\n",
    ")\n",
    "\n",
    "summary_task = Task(\n",
    "    description=\"\"\"Using the analysis provided by the researcher, create a concise yet comprehensive summary\n",
    "    of the 'Attention Is All You Need' paper. The summary should be suitable for graduate students in computer science.\"\"\",\n",
    "    expected_output=\"\"\"A clear, concise summary of the paper, including:\n",
    "    1. Main idea and significance of the Transformer architecture\n",
    "    2. Key components of the model (e.g., self-attention, positional encoding)\n",
    "    3. Advantages over previous approaches\n",
    "    4. Potential applications and impact on the field\n",
    "    The summary should be accessible to graduate students with basic knowledge of deep learning.\"\"\",\n",
    "    agent=writer\n",
    ")\n",
    "\n",
    "# Set up the CrewAI\n",
    "crew = Crew(\n",
    "    agents=[researcher, writer],\n",
    "    tasks=[analysis_task, summary_task],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf9cfb7-ca2d-4f47-8757-c8e3fb5a7b4c",
   "metadata": {},
   "source": [
    "## Run CrewAI Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "643b1917-0577-4a37-bdda-868ed2026840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m\u001B[95m [2024-07-31 10:12:50][DEBUG]: == Working Agent: Senior Computer Science Researcher\u001B[00m\n",
      "\u001B[1m\u001B[95m [2024-07-31 10:12:50][INFO]: == Starting Task: Conduct an in-depth analysis of the 'Attention Is All You Need' paper.\n",
      "    Focus on the key contributions, architectural details, and potential impacts on the field of NLP.\u001B[00m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new CrewAgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mTo conduct a comprehensive analysis of the 'Attention Is All You Need' paper, I will utilize the Research Paper Analysis tool to extract detailed insights on the specified criteria. The first step is to obtain an analysis of the key contributions and innovations of the Transformer architecture.\n",
      "\n",
      "Action: Research Paper Analysis\n",
      "Action Input: {\"query\": \"Key innovations of the Transformer architecture in 'Attention Is All You Need' paper\"}\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[95m \n",
      "\n",
      "The key innovations of the Transformer architecture in the \"Attention Is All You Need\" paper include:\n",
      "\n",
      "1. **Attention Mechanism**: The Transformer is the first sequence transduction model based entirely on attention mechanisms, replacing the recurrent layers commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "\n",
      "2. **Multi-Headed Self-Attention**: This allows the model to focus on different parts of the input sequence simultaneously, capturing various aspects of the data.\n",
      "\n",
      "3. **Elimination of Recurrence**: By eschewing recurrence, the Transformer can model dependencies without regard to their distance in the input or output sequences, which is a significant departure from traditional recurrent neural networks (RNNs).\n",
      "\n",
      "4. **Stacked Self-Attention and Fully Connected Layers**: Both the encoder and decoder in the Transformer architecture use stacked self-attention and point-wise, fully connected layers, enhancing the model's ability to process sequences in parallel.\n",
      "\n",
      "5. **Parameter-Free Position Representation**: This innovation helps the model understand the position of tokens in the sequence without relying on recurrent structures.\n",
      "\n",
      "These innovations collectively contribute to the Transformer's ability to be trained significantly faster than architectures based on recurrent or convolutional layers, especially for tasks like translation.\n",
      "\u001B[00m\n",
      "\u001B[32;1m\u001B[1;3mThought:\n",
      "The next step is to obtain a detailed explanation of the self-attention mechanism, which is a critical component of the Transformer architecture.\n",
      "\n",
      "Action: Research Paper Analysis\n",
      "Action Input: {\"query\": \"Detailed explanation of the self-attention mechanism in 'Attention Is All You Need' paper\"}\u001B[0m\u001B[95m \n",
      "\n",
      "I don't know. The provided context does not contain a detailed explanation of the self-attention mechanism as described in section 3.2 of the \"Attention Is All You Need\" paper.\n",
      "\u001B[00m\n",
      "\u001B[32;1m\u001B[1;3mThought:\n",
      "To fulfill the request comprehensively, I need to continue using the Research Paper Analysis tool to gather detailed information on the remaining aspects of the paper. Specifically, I will focus next on obtaining a detailed explanation of the self-attention mechanism.\n",
      "\n",
      "Action: Research Paper Analysis\n",
      "Action Input: {\"query\": \"Detailed explanation of the self-attention mechanism in the 'Attention Is All You Need' paper\"}\u001B[0m\u001B[95m \n",
      "\n",
      "I don't know. The provided context does not contain a detailed explanation of the self-attention mechanism as described in section 3.2 of the \"Attention Is All You Need\" paper.\n",
      "\u001B[00m\n",
      "\u001B[32;1m\u001B[1;3mThought:\n",
      "The previous attempt to obtain a detailed explanation of the self-attention mechanism did not succeed. I will try again to ensure I capture the necessary details about the self-attention mechanism from the \"Attention Is All You Need\" paper.\n",
      "\n",
      "Action: Research Paper Analysis\n",
      "Action Input: {\"query\": \"Self-attention mechanism in the 'Attention Is All You Need' paper\"}\u001B[0m\u001B[95m \n",
      "\n",
      "The self-attention mechanism described in the \"Attention Is All You Need\" paper is detailed in section 3.2. Self-attention, also known as intra-attention, is an attention mechanism that relates different positions of a single sequence to compute a representation of that sequence. It has been successfully applied to various tasks such as reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\n",
      "\u001B[00m\n",
      "\u001B[32;1m\u001B[1;3mThought:\n",
      "I have now gathered information about the self-attention mechanism in the \"Attention Is All You Need\" paper. The next step is to obtain a comparison of the Transformer architecture with previous architectures, such as RNNs and LSTMs.\n",
      "\n",
      "Action: Research Paper Analysis\n",
      "Action Input: {\"query\": \"Comparison of Transformer architecture with RNNs and LSTMs in 'Attention Is All You Need' paper\"}\u001B[0m\u001B[95m \n",
      "\n",
      "The \"Attention Is All You Need\" paper presents the Transformer architecture, which differs significantly from RNNs (Recurrent Neural Networks) and LSTMs (Long Short-Term Memory networks). Here are the key points of comparison based on the provided context:\n",
      "\n",
      "1. **Architecture**:\n",
      "   - **Transformer**: The Transformer model is based entirely on attention mechanisms, specifically multi-headed self-attention, and does not use any recurrent layers. It consists of an encoder-decoder structure where both the encoder and decoder are composed of stacked self-attention and point-wise, fully connected layers.\n",
      "   - **RNNs and LSTMs**: These models rely on recurrent layers to process sequences. RNNs and LSTMs use recurrent connections to maintain information across time steps, which helps in capturing temporal dependencies in the data.\n",
      "\n",
      "2. **Training Speed**:\n",
      "   - **Transformer**: The Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. This is because the self-attention mechanism allows for parallelization, whereas recurrent models process sequences sequentially, which is less efficient.\n",
      "   - **RNNs and LSTMs**: Training these models is generally slower due to the sequential nature of processing, which limits parallelization.\n",
      "\n",
      "3. **Dependency Modeling**:\n",
      "   - **Transformer**: The self-attention mechanism in the Transformer allows it to model dependencies without regard to their distance in the input or output sequences. This means it can capture long-range dependencies more effectively.\n",
      "   - **RNNs and LSTMs**: While these models can capture long-range dependencies, they often struggle with very long sequences due to issues like vanishing gradients (in RNNs) or the complexity of maintaining long-term dependencies (even though LSTMs mitigate this to some extent).\n",
      "\n",
      "In summary, the Transformer architecture offers advantages in terms of training speed and the ability to model long-range dependencies more effectively compared to RNNs and LSTMs, primarily due to its reliance on self-attention mechanisms instead of recurrent layers.\n",
      "\u001B[00m\n",
      "\u001B[32;1m\u001B[1;3mThought:\n",
      "The final step to complete the comprehensive analysis is to gather information on the potential implications for future NLP research and applications based on the \"Attention Is All You Need\" paper.\n",
      "\n",
      "Action: Research Paper Analysis\n",
      "Action Input: {\"query\": \"Potential implications for future NLP research and applications based on 'Attention Is All You Need' paper\"}\u001B[0m\u001B[95m \n",
      "\n",
      "Based on the provided context, I don't have enough information to directly answer the question about the potential implications for future NLP research and applications based on the \"Attention Is All You Need\" paper. The context mainly includes references to other works and some observations about attention heads in a neural network, but it does not provide a comprehensive discussion on the broader implications of the \"Attention Is All You Need\" paper.\n",
      "\n",
      "For a detailed analysis, one would typically consider how the introduction of the Transformer model, which relies solely on attention mechanisms, has influenced various aspects of NLP, such as improving the efficiency and effectiveness of language models, enabling better parallelization during training, and inspiring subsequent models like BERT and GPT. However, this specific context does not provide that information.\n",
      "\u001B[00m\n",
      "\u001B[32;1m\u001B[1;3mThought:\n",
      "I will make another attempt to gather information about the potential implications for future NLP research and applications based on the \"Attention Is All You Need\" paper. This information is crucial for completing the comprehensive analysis.\n",
      "\n",
      "Action: Research Paper Analysis\n",
      "Action Input: {\"query\": \"Implications for future NLP research and applications based on 'Attention Is All You Need' paper\"}\u001B[0m\u001B[95m \n",
      "\n",
      "I don't know. The provided context does not contain specific information about the implications for future NLP research and applications based on the \"Attention Is All You Need\" paper.\n",
      "\u001B[00m\n",
      "\u001B[32;1m\u001B[1;3mThought:\n",
      "The attempts to gather information about the potential implications for future NLP research and applications based on the \"Attention Is All You Need\" paper have not been successful. However, I have gathered substantial information on the key innovations of the Transformer architecture, a detailed explanation of the self-attention mechanism, and a comparison with previous architectures (RNNs and LSTMs). Using the available data, I can provide a comprehensive analysis focusing on these aspects, while also extrapolating the potential implications based on the known impact of the Transformer model in the NLP community.\n",
      "\n",
      "Final Answer:\n",
      "### Comprehensive Analysis Report on 'Attention Is All You Need' Paper\n",
      "\n",
      "#### 1. Key Innovations of the Transformer Architecture\n",
      "The \"Attention Is All You Need\" paper introduces several groundbreaking innovations in the Transformer architecture:\n",
      "- **Attention Mechanism**: The Transformer is the first sequence transduction model based entirely on attention mechanisms, replacing the recurrent layers commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "- **Multi-Headed Self-Attention**: This allows the model to focus on different parts of the input sequence simultaneously, capturing various aspects of the data.\n",
      "- **Elimination of Recurrence**: By eschewing recurrence, the Transformer can model dependencies without regard to their distance in the input or output sequences, which is a significant departure from traditional recurrent neural networks (RNNs).\n",
      "- **Stacked Self-Attention and Fully Connected Layers**: Both the encoder and decoder in the Transformer architecture use stacked self-attention and point-wise, fully connected layers, enhancing the model's ability to process sequences in parallel.\n",
      "- **Parameter-Free Position Representation**: This innovation helps the model understand the position of tokens in the sequence without relying on recurrent structures.\n",
      "\n",
      "These innovations collectively contribute to the Transformer's ability to be trained significantly faster than architectures based on recurrent or convolutional layers, especially for tasks like translation.\n",
      "\n",
      "#### 2. Detailed Explanation of the Self-Attention Mechanism\n",
      "The self-attention mechanism, also known as intra-attention, is a critical component of the Transformer architecture. It relates different positions of a single sequence to compute a representation of that sequence. This mechanism has been successfully applied to various tasks such as reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\n",
      "\n",
      "In the context of the Transformer:\n",
      "- The self-attention mechanism allows the model to weigh the importance of different words in a sequence when encoding a particular word.\n",
      "- Each word in the input sequence is transformed into three vectors: Query (Q), Key (K), and Value (V). The attention scores are computed using the dot product of the Query with all Keys, followed by a softmax function to obtain the weights.\n",
      "- The output is obtained by multiplying these weights with the Value vectors, allowing the model to focus on relevant parts of the sequence dynamically.\n",
      "\n",
      "This mechanism enables the Transformer to capture long-range dependencies more effectively than RNNs or LSTMs.\n",
      "\n",
      "#### 3. Comparison with Previous Architectures (RNNs, LSTMs)\n",
      "The Transformer architecture offers several advantages over traditional RNNs and LSTMs:\n",
      "- **Architecture**:\n",
      "  - **Transformer**: Based entirely on attention mechanisms, specifically multi-headed self-attention, without any recurrent layers. It uses an encoder-decoder structure with stacked self-attention and point-wise, fully connected layers.\n",
      "  - **RNNs and LSTMs**: These models rely on recurrent layers to process sequences. They use recurrent connections to maintain information across time steps, capturing temporal dependencies.\n",
      "- **Training Speed**:\n",
      "  - **Transformer**: Can be trained significantly faster due to the parallelizable nature of the self-attention mechanism.\n",
      "  - **RNNs and LSTMs**: Training is generally slower due to sequential processing.\n",
      "- **Dependency Modeling**:\n",
      "  - **Transformer**: Models dependencies without regard to their distance in the input or output sequences, effectively capturing long-range dependencies.\n",
      "  - **RNNs and LSTMs**: Can capture long-range dependencies but often struggle with very long sequences due to issues like vanishing gradients.\n",
      "\n",
      "#### 4. Potential Implications for Future NLP Research and Applications\n",
      "The introduction of the Transformer model has had profound implications for NLP research and applications:\n",
      "- **Efficiency and Effectiveness**: The Transformer's ability to be trained faster and capture long-range dependencies more effectively has led to significant improvements in various NLP tasks, including machine translation, text summarization, and question answering.\n",
      "- **Inspiration for Subsequent Models**: The success of the Transformer has inspired the development of other influential models, such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which have set new benchmarks in NLP.\n",
      "- **Parallelization**: The Transformer's reliance on self-attention mechanisms allows for better parallelization during training, making it more suitable for large-scale data processing and enabling the training of much larger models.\n",
      "- **Broad Applications**: Beyond NLP, the Transformer architecture has been adapted for various domains, including computer vision and reinforcement learning, demonstrating its versatility and impact on the broader field of machine learning.\n",
      "\n",
      "In conclusion, the \"Attention Is All You Need\" paper has introduced a transformative architecture that has reshaped the landscape of NLP and continues to influence ongoing research and development in the field.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\u001B[1m\u001B[92m [2024-07-31 10:13:44][DEBUG]: == [Senior Computer Science Researcher] Task output: ### Comprehensive Analysis Report on 'Attention Is All You Need' Paper\n",
      "\n",
      "#### 1. Key Innovations of the Transformer Architecture\n",
      "The \"Attention Is All You Need\" paper introduces several groundbreaking innovations in the Transformer architecture:\n",
      "- **Attention Mechanism**: The Transformer is the first sequence transduction model based entirely on attention mechanisms, replacing the recurrent layers commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "- **Multi-Headed Self-Attention**: This allows the model to focus on different parts of the input sequence simultaneously, capturing various aspects of the data.\n",
      "- **Elimination of Recurrence**: By eschewing recurrence, the Transformer can model dependencies without regard to their distance in the input or output sequences, which is a significant departure from traditional recurrent neural networks (RNNs).\n",
      "- **Stacked Self-Attention and Fully Connected Layers**: Both the encoder and decoder in the Transformer architecture use stacked self-attention and point-wise, fully connected layers, enhancing the model's ability to process sequences in parallel.\n",
      "- **Parameter-Free Position Representation**: This innovation helps the model understand the position of tokens in the sequence without relying on recurrent structures.\n",
      "\n",
      "These innovations collectively contribute to the Transformer's ability to be trained significantly faster than architectures based on recurrent or convolutional layers, especially for tasks like translation.\n",
      "\n",
      "#### 2. Detailed Explanation of the Self-Attention Mechanism\n",
      "The self-attention mechanism, also known as intra-attention, is a critical component of the Transformer architecture. It relates different positions of a single sequence to compute a representation of that sequence. This mechanism has been successfully applied to various tasks such as reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\n",
      "\n",
      "In the context of the Transformer:\n",
      "- The self-attention mechanism allows the model to weigh the importance of different words in a sequence when encoding a particular word.\n",
      "- Each word in the input sequence is transformed into three vectors: Query (Q), Key (K), and Value (V). The attention scores are computed using the dot product of the Query with all Keys, followed by a softmax function to obtain the weights.\n",
      "- The output is obtained by multiplying these weights with the Value vectors, allowing the model to focus on relevant parts of the sequence dynamically.\n",
      "\n",
      "This mechanism enables the Transformer to capture long-range dependencies more effectively than RNNs or LSTMs.\n",
      "\n",
      "#### 3. Comparison with Previous Architectures (RNNs, LSTMs)\n",
      "The Transformer architecture offers several advantages over traditional RNNs and LSTMs:\n",
      "- **Architecture**:\n",
      "  - **Transformer**: Based entirely on attention mechanisms, specifically multi-headed self-attention, without any recurrent layers. It uses an encoder-decoder structure with stacked self-attention and point-wise, fully connected layers.\n",
      "  - **RNNs and LSTMs**: These models rely on recurrent layers to process sequences. They use recurrent connections to maintain information across time steps, capturing temporal dependencies.\n",
      "- **Training Speed**:\n",
      "  - **Transformer**: Can be trained significantly faster due to the parallelizable nature of the self-attention mechanism.\n",
      "  - **RNNs and LSTMs**: Training is generally slower due to sequential processing.\n",
      "- **Dependency Modeling**:\n",
      "  - **Transformer**: Models dependencies without regard to their distance in the input or output sequences, effectively capturing long-range dependencies.\n",
      "  - **RNNs and LSTMs**: Can capture long-range dependencies but often struggle with very long sequences due to issues like vanishing gradients.\n",
      "\n",
      "#### 4. Potential Implications for Future NLP Research and Applications\n",
      "The introduction of the Transformer model has had profound implications for NLP research and applications:\n",
      "- **Efficiency and Effectiveness**: The Transformer's ability to be trained faster and capture long-range dependencies more effectively has led to significant improvements in various NLP tasks, including machine translation, text summarization, and question answering.\n",
      "- **Inspiration for Subsequent Models**: The success of the Transformer has inspired the development of other influential models, such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which have set new benchmarks in NLP.\n",
      "- **Parallelization**: The Transformer's reliance on self-attention mechanisms allows for better parallelization during training, making it more suitable for large-scale data processing and enabling the training of much larger models.\n",
      "- **Broad Applications**: Beyond NLP, the Transformer architecture has been adapted for various domains, including computer vision and reinforcement learning, demonstrating its versatility and impact on the broader field of machine learning.\n",
      "\n",
      "In conclusion, the \"Attention Is All You Need\" paper has introduced a transformative architecture that has reshaped the landscape of NLP and continues to influence ongoing research and development in the field.\n",
      "\n",
      "\u001B[00m\n",
      "\u001B[1m\u001B[95m [2024-07-31 10:13:44][DEBUG]: == Working Agent: Technical Writer and Educator\u001B[00m\n",
      "\u001B[1m\u001B[95m [2024-07-31 10:13:44][INFO]: == Starting Task: Using the analysis provided by the researcher, create a concise yet comprehensive summary\n",
      "    of the 'Attention Is All You Need' paper. The summary should be suitable for graduate students in computer science.\u001B[00m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new CrewAgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mI now can give a great answer.\n",
      "Final Answer: \n",
      "\n",
      "### Summary of 'Attention Is All You Need' for Graduate Students\n",
      "\n",
      "#### Introduction\n",
      "The \"Attention Is All You Need\" paper introduces the Transformer architecture, a novel model for sequence transduction tasks like machine translation. Unlike previous models that relied heavily on recurrent neural networks (RNNs) or long short-term memory networks (LSTMs), the Transformer is based entirely on self-attention mechanisms, which allows for more efficient training and better handling of long-range dependencies.\n",
      "\n",
      "#### Main Idea and Significance\n",
      "The primary innovation of the Transformer architecture is the replacement of recurrent layers with multi-headed self-attention mechanisms. This design enables the model to process entire sequences in parallel, significantly speeding up training times and improving performance on various natural language processing (NLP) tasks. The significance of this architecture lies in its ability to handle dependencies across sequences without the limitations imposed by sequential processing in RNNs and LSTMs.\n",
      "\n",
      "#### Key Components\n",
      "1. **Self-Attention Mechanism**: This is the core of the Transformer. It allows the model to weigh the importance of different words in a sequence when encoding a particular word. Each word in the input sequence is transformed into three vectors: Query (Q), Key (K), and Value (V). The attention scores are calculated using the dot product of the Query with all Keys, followed by a softmax function to obtain the weights. These weights are then used to compute a weighted sum of the Value vectors, enabling the model to focus on relevant parts of the sequence dynamically.\n",
      "\n",
      "2. **Multi-Headed Self-Attention**: This extends the self-attention mechanism by allowing the model to focus on different parts of the sequence simultaneously. Each \"head\" processes the sequence independently, capturing various aspects of the data. The outputs of these heads are then concatenated and linearly transformed, providing a richer representation of the sequence.\n",
      "\n",
      "3. **Positional Encoding**: Since the Transformer does not use recurrence or convolution, it employs positional encodings to inject information about the position of tokens in the sequence. These encodings are added to the input embeddings to help the model understand the order of the words.\n",
      "\n",
      "4. **Stacked Layers**: Both the encoder and decoder consist of multiple layers of self-attention and fully connected layers. The encoder processes the input sequence to create a rich representation, while the decoder generates the output sequence by attending to the encoder's output and its own previous predictions.\n",
      "\n",
      "#### Advantages Over Previous Approaches\n",
      "1. **Training Speed**: The parallelizable nature of the self-attention mechanism allows the Transformer to be trained much faster than RNNs and LSTMs, which require sequential processing.\n",
      "2. **Long-Range Dependency Modeling**: The Transformer can capture dependencies between distant words more effectively than RNNs and LSTMs, which often struggle with long-range dependencies due to vanishing gradient issues.\n",
      "3. **Scalability**: The architecture's reliance on self-attention allows for better parallelization, making it more suitable for large-scale data processing and enabling the training of much larger models.\n",
      "\n",
      "#### Potential Applications and Impact\n",
      "The Transformer architecture has had a profound impact on the field of NLP and beyond:\n",
      "1. **NLP Tasks**: It has led to significant improvements in tasks like machine translation, text summarization, and question answering.\n",
      "2. **Inspired Models**: The success of the Transformer has inspired the development of other influential models such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which have set new benchmarks in NLP.\n",
      "3. **Cross-Domain Applications**: The architecture has been adapted for various domains, including computer vision and reinforcement learning, demonstrating its versatility and broad applicability.\n",
      "\n",
      "#### Conclusion\n",
      "The \"Attention Is All You Need\" paper has introduced a revolutionary architecture that has reshaped the landscape of NLP and continues to influence ongoing research and development in the field. The Transformer's efficiency, effectiveness, and scalability make it a cornerstone of modern deep learning models.\n",
      "\n",
      "This summary covers the key aspects of the Transformer architecture, its innovations, and its impact, making it accessible to graduate students with a basic understanding of deep learning.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\u001B[1m\u001B[92m [2024-07-31 10:13:59][DEBUG]: == [Technical Writer and Educator] Task output: ### Summary of 'Attention Is All You Need' for Graduate Students\n",
      "\n",
      "#### Introduction\n",
      "The \"Attention Is All You Need\" paper introduces the Transformer architecture, a novel model for sequence transduction tasks like machine translation. Unlike previous models that relied heavily on recurrent neural networks (RNNs) or long short-term memory networks (LSTMs), the Transformer is based entirely on self-attention mechanisms, which allows for more efficient training and better handling of long-range dependencies.\n",
      "\n",
      "#### Main Idea and Significance\n",
      "The primary innovation of the Transformer architecture is the replacement of recurrent layers with multi-headed self-attention mechanisms. This design enables the model to process entire sequences in parallel, significantly speeding up training times and improving performance on various natural language processing (NLP) tasks. The significance of this architecture lies in its ability to handle dependencies across sequences without the limitations imposed by sequential processing in RNNs and LSTMs.\n",
      "\n",
      "#### Key Components\n",
      "1. **Self-Attention Mechanism**: This is the core of the Transformer. It allows the model to weigh the importance of different words in a sequence when encoding a particular word. Each word in the input sequence is transformed into three vectors: Query (Q), Key (K), and Value (V). The attention scores are calculated using the dot product of the Query with all Keys, followed by a softmax function to obtain the weights. These weights are then used to compute a weighted sum of the Value vectors, enabling the model to focus on relevant parts of the sequence dynamically.\n",
      "\n",
      "2. **Multi-Headed Self-Attention**: This extends the self-attention mechanism by allowing the model to focus on different parts of the sequence simultaneously. Each \"head\" processes the sequence independently, capturing various aspects of the data. The outputs of these heads are then concatenated and linearly transformed, providing a richer representation of the sequence.\n",
      "\n",
      "3. **Positional Encoding**: Since the Transformer does not use recurrence or convolution, it employs positional encodings to inject information about the position of tokens in the sequence. These encodings are added to the input embeddings to help the model understand the order of the words.\n",
      "\n",
      "4. **Stacked Layers**: Both the encoder and decoder consist of multiple layers of self-attention and fully connected layers. The encoder processes the input sequence to create a rich representation, while the decoder generates the output sequence by attending to the encoder's output and its own previous predictions.\n",
      "\n",
      "#### Advantages Over Previous Approaches\n",
      "1. **Training Speed**: The parallelizable nature of the self-attention mechanism allows the Transformer to be trained much faster than RNNs and LSTMs, which require sequential processing.\n",
      "2. **Long-Range Dependency Modeling**: The Transformer can capture dependencies between distant words more effectively than RNNs and LSTMs, which often struggle with long-range dependencies due to vanishing gradient issues.\n",
      "3. **Scalability**: The architecture's reliance on self-attention allows for better parallelization, making it more suitable for large-scale data processing and enabling the training of much larger models.\n",
      "\n",
      "#### Potential Applications and Impact\n",
      "The Transformer architecture has had a profound impact on the field of NLP and beyond:\n",
      "1. **NLP Tasks**: It has led to significant improvements in tasks like machine translation, text summarization, and question answering.\n",
      "2. **Inspired Models**: The success of the Transformer has inspired the development of other influential models such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which have set new benchmarks in NLP.\n",
      "3. **Cross-Domain Applications**: The architecture has been adapted for various domains, including computer vision and reinforcement learning, demonstrating its versatility and broad applicability.\n",
      "\n",
      "#### Conclusion\n",
      "The \"Attention Is All You Need\" paper has introduced a revolutionary architecture that has reshaped the landscape of NLP and continues to influence ongoing research and development in the field. The Transformer's efficiency, effectiveness, and scalability make it a cornerstone of modern deep learning models.\n",
      "\n",
      "This summary covers the key aspects of the Transformer architecture, its innovations, and its impact, making it accessible to graduate students with a basic understanding of deep learning.\n",
      "\n",
      "\u001B[00m\n",
      "CrewAI Analysis Result:\n",
      "### Summary of 'Attention Is All You Need' for Graduate Students\n",
      "\n",
      "#### Introduction\n",
      "The \"Attention Is All You Need\" paper introduces the Transformer architecture, a novel model for sequence transduction tasks like machine translation. Unlike previous models that relied heavily on recurrent neural networks (RNNs) or long short-term memory networks (LSTMs), the Transformer is based entirely on self-attention mechanisms, which allows for more efficient training and better handling of long-range dependencies.\n",
      "\n",
      "#### Main Idea and Significance\n",
      "The primary innovation of the Transformer architecture is the replacement of recurrent layers with multi-headed self-attention mechanisms. This design enables the model to process entire sequences in parallel, significantly speeding up training times and improving performance on various natural language processing (NLP) tasks. The significance of this architecture lies in its ability to handle dependencies across sequences without the limitations imposed by sequential processing in RNNs and LSTMs.\n",
      "\n",
      "#### Key Components\n",
      "1. **Self-Attention Mechanism**: This is the core of the Transformer. It allows the model to weigh the importance of different words in a sequence when encoding a particular word. Each word in the input sequence is transformed into three vectors: Query (Q), Key (K), and Value (V). The attention scores are calculated using the dot product of the Query with all Keys, followed by a softmax function to obtain the weights. These weights are then used to compute a weighted sum of the Value vectors, enabling the model to focus on relevant parts of the sequence dynamically.\n",
      "\n",
      "2. **Multi-Headed Self-Attention**: This extends the self-attention mechanism by allowing the model to focus on different parts of the sequence simultaneously. Each \"head\" processes the sequence independently, capturing various aspects of the data. The outputs of these heads are then concatenated and linearly transformed, providing a richer representation of the sequence.\n",
      "\n",
      "3. **Positional Encoding**: Since the Transformer does not use recurrence or convolution, it employs positional encodings to inject information about the position of tokens in the sequence. These encodings are added to the input embeddings to help the model understand the order of the words.\n",
      "\n",
      "4. **Stacked Layers**: Both the encoder and decoder consist of multiple layers of self-attention and fully connected layers. The encoder processes the input sequence to create a rich representation, while the decoder generates the output sequence by attending to the encoder's output and its own previous predictions.\n",
      "\n",
      "#### Advantages Over Previous Approaches\n",
      "1. **Training Speed**: The parallelizable nature of the self-attention mechanism allows the Transformer to be trained much faster than RNNs and LSTMs, which require sequential processing.\n",
      "2. **Long-Range Dependency Modeling**: The Transformer can capture dependencies between distant words more effectively than RNNs and LSTMs, which often struggle with long-range dependencies due to vanishing gradient issues.\n",
      "3. **Scalability**: The architecture's reliance on self-attention allows for better parallelization, making it more suitable for large-scale data processing and enabling the training of much larger models.\n",
      "\n",
      "#### Potential Applications and Impact\n",
      "The Transformer architecture has had a profound impact on the field of NLP and beyond:\n",
      "1. **NLP Tasks**: It has led to significant improvements in tasks like machine translation, text summarization, and question answering.\n",
      "2. **Inspired Models**: The success of the Transformer has inspired the development of other influential models such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which have set new benchmarks in NLP.\n",
      "3. **Cross-Domain Applications**: The architecture has been adapted for various domains, including computer vision and reinforcement learning, demonstrating its versatility and broad applicability.\n",
      "\n",
      "#### Conclusion\n",
      "The \"Attention Is All You Need\" paper has introduced a revolutionary architecture that has reshaped the landscape of NLP and continues to influence ongoing research and development in the field. The Transformer's efficiency, effectiveness, and scalability make it a cornerstone of modern deep learning models.\n",
      "\n",
      "This summary covers the key aspects of the Transformer architecture, its innovations, and its impact, making it accessible to graduate students with a basic understanding of deep learning.\n"
     ]
    }
   ],
   "source": [
    "result = crew.kickoff()\n",
    "\n",
    "print(\"CrewAI Analysis Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9970d9e-ca1b-41da-b7f2-18321149ae29",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates an advanced RAG pipeline using CrewAI to analyze research papers. The pipeline includes:\n",
    "\n",
    "1. Loading and processing PDF documents\n",
    "2. Embedding and storing document chunks in a Qdrant vector database\n",
    "3. Setting up a RAG pipeline with OpenAI's GPT-4o\n",
    "4. Using CrewAI to orchestrate a detailed analysis and summary of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23da4c5-8e5d-4ffb-b831-6818c3641e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca99177b-7460-4a94-99c6-18aae7d24218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
